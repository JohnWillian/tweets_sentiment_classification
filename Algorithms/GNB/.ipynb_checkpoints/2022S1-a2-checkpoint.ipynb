{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Project, we will implement a **Naive Bayes** classifier, apply it to various classification datasets, and explore evaluation paradigms as well as the impact of individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function open a csv file and read the data into a useable format\n",
    "def preprocess(filename):\n",
    "    data = open(filename).readlines()\n",
    "    datalines = []\n",
    "    for line in data:\n",
    "        datalines.append(line.strip().split(\",\"))\n",
    "    header = datalines[0]\n",
    "    datalines = datalines[1:]\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in datalines:\n",
    "        features.append([float(i) for i in line[2:]])\n",
    "        labels.append(line[1])\n",
    "        \n",
    "    return (features,labels,header[2:])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function build a supervised NB model\n",
    "import math\n",
    "\n",
    "def train(features, labels,header):\n",
    "    total = len(labels)\n",
    "    \n",
    "    priors = dict()\n",
    "    for label in labels:\n",
    "        if label in priors.keys():\n",
    "            priors[label] += 1\n",
    "        else:\n",
    "            priors.setdefault(label, 1)\n",
    "        \n",
    "    attri = dict()\n",
    "    likelihoods = dict()\n",
    "    \n",
    "    for name in header:\n",
    "        attri[name] = dict()\n",
    "        likelihoods[name] = dict()\n",
    "        for item in priors:\n",
    "            attri[name][item] = []\n",
    "            likelihoods[name][item] = []\n",
    "            \n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        m = len(features[i])\n",
    "        for j in range(m):\n",
    "            attri[header[j]][labels[i]].append(features[i][j])\n",
    "    for name in header:\n",
    "        for item in priors:\n",
    "            mean = sum(attri[name][item])/len(attri[name][item])\n",
    "            standard_deviation = math.sqrt(sum([(value - mean)**2 for value in attri[name][item]])/len(attri[name][item]))\n",
    "            likelihoods[name][item] = [mean, standard_deviation]\n",
    "    return likelihoods\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predict the class for a set of instances, based on a trained model\n",
    "def predict(likelihoods,test,labels,header):\n",
    "    total = len(labels)\n",
    "    \n",
    "    priors = dict()\n",
    "    for label in labels:\n",
    "        if label in priors.keys():\n",
    "            priors[label] += 1\n",
    "        else:\n",
    "            priors.setdefault(label, 1)\n",
    "    \n",
    "    for item in priors:\n",
    "        priors[item] = math.log(priors[item] / total)\n",
    "    \n",
    "    predict_list = []\n",
    "        \n",
    "    for line in test:\n",
    "        n = len(line)\n",
    "        gause_value = dict()\n",
    "        for item in priors:\n",
    "            temp = 0\n",
    "            for i in range(n):\n",
    "                x = line[i]\n",
    "                mean = likelihoods[header[i]][item][0]\n",
    "                standard_devi = likelihoods[header[i]][item][1]\n",
    "                if standard_devi == 0:\n",
    "                    gaussian = 0\n",
    "                else:\n",
    "                    gaussian = (-1/2)*math.log(math.pi)-(1/2)*math.log(2)-math.log(standard_devi)-(1/2*((x-mean)/standard_devi)**2)*math.log(math.exp(1))\n",
    "                temp += gaussian\n",
    "            gause_value[item] = priors[item] + temp\n",
    "        predict_list.append(gause_value)\n",
    "\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluate a set of predictions\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate(predict_list, labels):\n",
    "    pred = []\n",
    "    true = labels\n",
    "\n",
    "    for i in predict_list:\n",
    "        pred.append(max(i, key=i.get))\n",
    "\n",
    "        \n",
    "## Calculate accuracy, precision, recall, and f_score with sklearn\n",
    "        \n",
    "    accuracy = accuracy_score(true, pred)\n",
    "    precision = precision_score(true, pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true, pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true, pred, average='weighted', zero_division=0)\n",
    "    c_m = confusion_matrix(true, pred)\n",
    "    return (accuracy,precision,recall,f1,c_m)\n",
    "\n",
    "\n",
    "\n",
    "## Calculate accuracy, precision, recall, and f_score without sklearn\n",
    "\n",
    "#     total = len(labels)\n",
    "#     priors = Counter(labels)\n",
    "#     c = len(priors)\n",
    "#     precision = 0\n",
    "#     recall = 0\n",
    "#     accuracy = 0\n",
    "    \n",
    "#     for i in range(total):\n",
    "#         if pre[i] == labels[i]:\n",
    "#             accuracy += 1\n",
    "        \n",
    "        \n",
    "#     for item in priors:\n",
    "#         TP = 0\n",
    "#         TN = 0\n",
    "#         FP = 0\n",
    "#         FN = 0\n",
    "        \n",
    "#         for i in range(total):\n",
    "#             if pre[i] == labels[i]:\n",
    "#                 if labels[i] == item:\n",
    "#                     TP += 1\n",
    "#                 else:\n",
    "#                     TN += 1\n",
    "#             else:\n",
    "#                 if labels[i] == item:\n",
    "#                     FN += 1\n",
    "#                 elif pre[i] == item:\n",
    "#                     FP += 1\n",
    "#                 else:\n",
    "#                     TN += 1\n",
    "#         precision = precision + (TP/(TP+FP))\n",
    "#         recall = recall + (TP/(TP+FN))\n",
    "        \n",
    "#     accuracy = accuracy / total\n",
    "#     macro_precision = precision / c\n",
    "#     macro_recall = recall / c\n",
    "#     b = 1\n",
    "#     f_score = ((1+ b**2)*macro_precision*macro_recall) / ((b**2)*macro_precision + macro_recall)\n",
    "    \n",
    "#     return (accuracy,macro_precision,macro_recall,f_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[555  80]\n",
      " [128 237]] \n",
      "\n",
      "Accuracy:  0.792\n",
      "Precision:  0.7888820429447003\n",
      "Recall:  0.792\n",
      "F_score:  0.7884679088105591\n",
      "\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]:  [109.0, 8.0, 0.0, 0.0, 0.0, 0.0, 12.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 3.0] [309.0, 35.0, 0.0, 0.0, 2.0, 1.0, 17.0, 0.0, 7.0, 5.0, 16.0, 0.0, 0.0, 0.0, 7.0, 0.0, 7.0, 0.0, 0.0, 19.0, 1.0, 0.0, 10.0] [149.0, 15.0, 0.0, 0.0, 0.0, 1.0, 4.0, 0.0, 4.0, 0.0, 7.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 2.0]\n",
      "\n",
      "Number of instances (N):  1000\n",
      "Number of features (F):  23\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'objective': -150.516960152298, 'subjective': -82.82526101853317}\n",
      "Predicted class ID for instance N-3:  (1, 'subjective')\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'objective': -66.35717394938793, 'subjective': -83.32555354318245}\n",
      "Predicted class ID for instance N-2:  (0, 'objective')\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'objective': -75.21844182521214, 'subjective': -74.89860954227957}\n",
      "Predicted class ID for instance N-1:  (1, 'subjective')\n"
     ]
    }
   ],
   "source": [
    "# This cell act as our \"main\" function where we call the above functions \n",
    "# on the full OBJECTIVITY data set, and print the evaluation score.\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply NB model to the OBJECTIVITY data\n",
    "\n",
    "features, labels, header = preprocess(\"objectivity.csv\")\n",
    "likelihoods = train(features, labels, header)\n",
    "test = features\n",
    "predict_list = predict(likelihoods, test, labels, header)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "accuracy, precision, recall, f_score, c_m = evaluate(predict_list, labels)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(c_m, \"\\n\")\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F_score: \",f_score)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "N = len(features)\n",
    "F = len(header)\n",
    "L = len(Counter(labels))\n",
    "\n",
    "N_3 = zip(predict_list[N-3].values(), predict_list[N-3].keys())\n",
    "N_3_ID, N_3_value = max(enumerate(N_3), key=lambda x: x[1][0])\n",
    "\n",
    "N_2 = zip(predict_list[N-2].values(), predict_list[N-2].keys())\n",
    "N_2_ID, N_2_value = max(enumerate(N_2), key=lambda x: x[1][0])\n",
    "\n",
    "N_1 = zip(predict_list[N-1].values(), predict_list[N-1].keys())\n",
    "N_1_ID, N_1_value = max(enumerate(N_1), key=lambda x: x[1][0])\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", features[0],features[1],features[2])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", N)\n",
    "print(\"Number of features (F): \", F)\n",
    "print(\"Number of labels (L): \", L)\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", predict_list[N-3])\n",
    "print(\"Predicted class ID for instance N-3: \", (N_3_ID,N_3_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", predict_list[N-2])\n",
    "print(\"Predicted class ID for instance N-2: \", (N_2_ID,N_2_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", predict_list[N-1])\n",
    "print(\"Predicted class ID for instance N-1: \", (N_1_ID,N_1_value[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1817   96]\n",
      " [ 389  198]] \n",
      "\n",
      "Accuracy:  0.806\n",
      "Precision:  0.7883973393527856\n",
      "Recall:  0.806\n",
      "F_score:  0.7806400438266895\n",
      "\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]:  [31.0, 142470.0, 13.0, 0.0, 0.0, 40.0] [31.0, 323069.0, 9.0, 0.0, 0.0, 20.0] [25.0, 122489.0, 13.0, 0.0, 1726.0, 60.0]\n",
      "\n",
      "Number of instances (N):  2500\n",
      "Number of features (F):  6\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'<=50K': -38.94392814438511, '>50K': -41.45333754859243}\n",
      "Predicted class ID for instance N-3:  (0, '<=50K')\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'<=50K': -36.74339621680319, '>50K': -41.86399859812137}\n",
      "Predicted class ID for instance N-2:  (0, '<=50K')\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'<=50K': -36.405924581549826, '>50K': -40.77617081107401}\n",
      "Predicted class ID for instance N-1:  (0, '<=50K')\n"
     ]
    }
   ],
   "source": [
    "# This cell act as our \"main\" function where we call the above functions \n",
    "# on the full ADULT data set, and print the evaluation score.\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply NB model to the ADULT data\n",
    "\n",
    "features, labels, header = preprocess(\"adult.csv\")\n",
    "likelihoods = train(features, labels, header)\n",
    "test = features\n",
    "predict_list = predict(likelihoods, test, labels, header)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "accuracy, precision, recall, f_score, c_m = evaluate(predict_list, labels)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(c_m, \"\\n\")\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F_score: \",f_score)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "N = len(features)\n",
    "F = len(header)\n",
    "L = len(Counter(labels))\n",
    "\n",
    "N_3 = zip(predict_list[N-3].values(), predict_list[N-3].keys())\n",
    "N_3_ID, N_3_value = max(enumerate(N_3), key=lambda x: x[1][0])\n",
    "\n",
    "N_2 = zip(predict_list[N-2].values(), predict_list[N-2].keys())\n",
    "N_2_ID, N_2_value = max(enumerate(N_2), key=lambda x: x[1][0])\n",
    "\n",
    "N_1 = zip(predict_list[N-1].values(), predict_list[N-1].keys())\n",
    "N_1_ID, N_1_value = max(enumerate(N_1), key=lambda x: x[1][0])\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", features[0],features[1],features[2])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", N)\n",
    "print(\"Number of features (F): \", F)\n",
    "print(\"Number of labels (L): \", L)\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", predict_list[N-3])\n",
    "print(\"Predicted class ID for instance N-3: \", (N_3_ID,N_3_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", predict_list[N-2])\n",
    "print(\"Predicted class ID for instance N-2: \", (N_2_ID,N_2_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", predict_list[N-1])\n",
    "print(\"Predicted class ID for instance N-1: \", (N_1_ID,N_1_value[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[11  8  5  2  4 14  0]\n",
      " [ 4 13 17  6 24 24  0]\n",
      " [ 2  9 59  3 46 31  7]\n",
      " [ 1  3  4  3  1  3  1]\n",
      " [ 3  9 28  0 42 29  1]\n",
      " [17  6 33  7 44 96  5]\n",
      " [ 2  2  2  1  4 11  6]] \n",
      "\n",
      "Accuracy:  0.35222052067381315\n",
      "Precision:  0.35629210861982685\n",
      "Recall:  0.35222052067381315\n",
      "F_score:  0.3496703185018923\n",
      "\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]:  [18.0, 50.0, 239.554, 97.0, 1.0, 0.0, 98.0, 178.0, 31.0] [18.0, 38.0, 239.554, 97.0, 0.0, 0.0, 89.0, 170.0, 31.0] [13.0, 33.0, 239.554, 97.0, 2.0, 1.0, 90.0, 172.0, 30.0]\n",
      "\n",
      "Number of instances (N):  653\n",
      "Number of features (F):  9\n",
      "Number of labels (L):  7\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'0': -35.04508390851188, '2': -45.433974146408886, '8': -36.67209742868353, '>24': -68.21715642872516, '1': -49.765047685902246, '3': -43.25400339703182, '24': -228.76823170460926}\n",
      "Predicted class ID for instance N-3:  (0, '0')\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'0': -28.104802702828287, '2': -29.32670678731827, '8': -27.911295474225774, '>24': -33.735479963486256, '1': -29.63235147892367, '3': -28.593025949957227, '24': -40.32983981346999}\n",
      "Predicted class ID for instance N-2:  (2, '8')\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'0': -28.507133651429974, '2': -27.55527829704712, '8': -28.06979866045698, '>24': -27.960390382825782, '1': -26.896519767891842, '3': -28.131122510036306, '24': -31.501472230288726}\n",
      "Predicted class ID for instance N-1:  (4, '1')\n"
     ]
    }
   ],
   "source": [
    "# This cell act as our \"main\" function where we call the above functions \n",
    "# on the full ABSENTEEISM data set, and print the evaluation score.\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply NB model to the ABSENTEEISM data\n",
    "\n",
    "features, labels, header = preprocess(\"absenteeism.csv\")\n",
    "likelihoods = train(features, labels, header)\n",
    "test = features\n",
    "predict_list = predict(likelihoods, test, labels, header)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "accuracy, precision, recall, f_score, c_m = evaluate(predict_list, labels)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(c_m, \"\\n\")\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F_score: \",f_score)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "N = len(features)\n",
    "F = len(header)\n",
    "L = len(Counter(labels))\n",
    "\n",
    "N_3 = zip(predict_list[N-3].values(), predict_list[N-3].keys())\n",
    "N_3_ID, N_3_value = max(enumerate(N_3), key=lambda x: x[1][0])\n",
    "\n",
    "N_2 = zip(predict_list[N-2].values(), predict_list[N-2].keys())\n",
    "N_2_ID, N_2_value = max(enumerate(N_2), key=lambda x: x[1][0])\n",
    "\n",
    "N_1 = zip(predict_list[N-1].values(), predict_list[N-1].keys())\n",
    "N_1_ID, N_1_value = max(enumerate(N_1), key=lambda x: x[1][0])\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", features[0],features[1],features[2])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", N)\n",
    "print(\"Number of features (F): \", F)\n",
    "print(\"Number of labels (L): \", L)\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", predict_list[N-3])\n",
    "print(\"Predicted class ID for instance N-3: \", (N_3_ID,N_3_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", predict_list[N-2])\n",
    "print(\"Predicted class ID for instance N-2: \", (N_2_ID,N_2_value[1]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", predict_list[N-1])\n",
    "print(\"Predicted class ID for instance N-1: \", (N_1_ID,N_1_value[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name:  objectivity.csv\n",
      "avg_accuracy:  0.788\n",
      "avg_precision:  0.7861520714461183\n",
      "avg_recall:  0.788\n",
      "avg_f_score:  0.7839687710960465\n",
      "\n",
      "\n",
      "File name:  adult.csv\n",
      "avg_accuracy:  0.8048\n",
      "avg_precision:  0.7870905780147466\n",
      "avg_recall:  0.8048\n",
      "avg_f_score:  0.7793134267919625\n",
      "\n",
      "\n",
      "File name:  absenteeism.csv\n",
      "avg_accuracy:  0.31867132867132864\n",
      "avg_precision:  0.31452519967829023\n",
      "avg_recall:  0.31867132867132864\n",
      "avg_f_score:  0.3095787508522922\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "file_name = [\"objectivity.csv\",\"adult.csv\",\"absenteeism.csv\"]\n",
    "\n",
    "for name in file_name:\n",
    "    features, labels, header = preprocess(name)\n",
    "    avg_accuracy, avg_precision, avg_recall, avg_f_score = [],[],[],[]\n",
    "    cross = 10\n",
    "    skf = StratifiedKFold(n_splits=cross, shuffle=True)\n",
    "\n",
    "    for x, y in skf.split(features, labels):\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        test_features = []\n",
    "        test_labels = []\n",
    "\n",
    "        for i in x:\n",
    "            train_features.append(features[i])\n",
    "            train_labels.append(labels[i])\n",
    "        for j in y:\n",
    "            test_features.append(features[j])\n",
    "            test_labels.append(labels[j])\n",
    "\n",
    "        likelihoods = train(train_features, train_labels, header)\n",
    "        predict_list = predict(likelihoods, test_features, labels, header)\n",
    "\n",
    "        accuracy, precision, recall, f_score, c_m = evaluate(predict_list, test_labels)\n",
    "\n",
    "        avg_accuracy.append(accuracy)\n",
    "        avg_precision.append(precision)\n",
    "        avg_recall.append(recall)\n",
    "        avg_f_score.append(f_score)\n",
    "        \n",
    "    print(\"File name: \", name)\n",
    "    print(\"avg_accuracy: \", np.array(avg_accuracy).mean())\n",
    "    print(\"avg_precision: \", np.array(avg_precision).mean())\n",
    "    print(\"avg_recall: \", np.array(avg_recall).mean())\n",
    "    print(\"avg_f_score: \", np.array(avg_f_score).mean())\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
